# The Agent Learning Cycle: A Memory Curation Guide

**Status:** Proposed
**Version:** 1.0

## 1. Philosophy: From Tool to Digital Artisan

An AI agent becomes truly powerful not just by executing a task, but by **learning from its work**. The Conductor is designed to facilitate a continuous learning cycle, transforming agents from mere code generation tools into "digital artisans" that refine their craft over time.

This guide describes the workflow for agent **memory curation**. Curation is a **human-supervised** step, ensuring that only high-quality knowledge is added to the agent's long-term memory. This prevents the degradation of its behavior and ensures constant improvement.

## 2. The Agile Curation Workflow

This process integrates into an agile development workflow (Scrum/Kanban) and is centered around the review of Pull Requests (PRs).

### Step 1: The Story and the Implementation Plan (The "What?")

The cycle begins with a User Story (e.g., "Create CRUD for `Dog`"). Based on it, the developer creates an `implementation_plan.yaml` detailing the tasks for the `Creator` and `TestCreator` agents to generate the initial functionality.

### Step 2: The "Inner Loop" - Code Generation and PR

The developer runs Conductor with the above plan. Conductor orchestrates the agents to generate the feature's code. Once the automated tests pass locally, the developer commits the code to a *feature branch* and opens a **Pull Request** to the `main` branch.

### Step 3: The "Outer Loop" - AI-Assisted PR Review

The creation of the PR triggers a CI/CD pipeline that executes a specialized review plan: `pr_review_plan.yaml`. This plan uses **Reviewer Agents**:

1.  **Task: `Execute_QA_Checks`**
    *   **Assigned Agent:** `Claude_QA_Agent`
    *   **Responsibility:** To run all unit and integration tests related to the PR's code. The agent posts a comment on the PR with the result: SUCCESS or FAILURE.

2.  **Task: `Generate_Learning_Summary`**
    *   **Assigned Agent:** `Architect_Agent` (e.g., a Gemini-powered agent)
    *   **Responsibility:** To analyze the code *diff* in the PR, looking for patterns, adherence to architecture, and opportunities for improvement. This task depends on the success of `Execute_QA_Checks`.
    *   **Output Artifact:** The agent generates a report named `learning_summary.md` and attaches it to the PR. This report is the basis for the agent's learning.

### Step 4: Human Curation (The Critical Link)

With the PR open, the human reviewer (Tech Lead or another developer) has at their disposal:
1.  The source code.
2.  The test results from the `Claude_QA_Agent`.
3.  The `learning_summary.md` from the `Architect_Agent`.

The human acts as the **Memory Curator**. They analyze the suggestions in the `learning_summary.md` and, using their professional judgment, decide what constitutes valid learning. They then **copy and paste** the learned lessons into the memory files of the original agents:

*   **Successful Patterns** are added to the relevant agent's `memory/context.md` to reinforce good behaviors.
*   **"Scars" (Errors and Bad Practices)** are added to `memory/avoid_patterns.md` so the agent does not repeat the same mistake.

After curating the memory and approving the code, the PR is merged.

### Step 5: The Next Cycle (Learning in Action)

In the following Sprint, a new related story is started (e.g., "Add endpoint to search for `Dog` by breed"). When Conductor is run again, the `Creator` agents will load their updated memories. They will now be aware of the new patterns to follow and the new "scars" to avoid, resulting in higher-quality code, autonomously.

---

## 3. Definition of New Components

*   **QA Agent (`QA_Agent`):**
    *   An agent specialized in running test suites and reporting results in a binary (pass/fail) manner. It does not analyze code quality, only its functional correctness.

*   **Architect Agent (`Architect_Agent`):**
    *   An agent specialized in code analysis, software architecture, and best practices. It uses a language model with a large context window (like Gemini) to analyze the PR as a whole. Its output is not code, but an analysis report.

*   **Review Plan (`pr_review_plan.yaml`):**
    *   A special type of implementation plan that does not generate code, but orchestrates Reviewer Agents to analyze a Pull Request.

*   **Learning Summary (`learning_summary.md`):**
    *   The artifact generated by the `Architect_Agent`. It contains a structured analysis of the code, divided into sections like "Successful Patterns to Reinforce" and "Scars to Memorize".