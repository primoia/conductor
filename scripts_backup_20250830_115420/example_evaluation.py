#!/usr/bin/env python3
"""
Exemplo de AvaliaÃ§Ã£o de Agentes Conductor

Este script demonstra como usar o sistema de avaliaÃ§Ã£o de agentes
de forma programÃ¡tica, sem usar o script shell.

Autor: Conductor Team
VersÃ£o: 1.0
Data: 2025-01-16
"""

import sys
import os
from pathlib import Path

# Adicionar o diretÃ³rio do script ao path
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir))

from agent_evaluator import AgentEvaluator, TestCase

def main():
    """FunÃ§Ã£o principal do exemplo"""
    print("ğŸ¤– Sistema de AvaliaÃ§Ã£o de Agentes Conductor")
    print("=" * 50)
    
    # Configurar caminho do Conductor
    conductor_root = script_dir.parent
    print(f"ğŸ“ DiretÃ³rio do Conductor: {conductor_root}")
    
    # Criar avaliador
    evaluator = AgentEvaluator(str(conductor_root))
    
    # Mostrar agentes disponÃ­veis
    print("\nğŸ“‹ Agentes disponÃ­veis para avaliaÃ§Ã£o:")
    for agent_id in evaluator.test_cases.keys():
        print(f"  - {agent_id}")
    
    # Executar avaliaÃ§Ã£o de exemplo
    print("\nğŸš€ Executando avaliaÃ§Ã£o de exemplo...")
    
    # Avaliar AgentCreator_Agent
    print("\n1ï¸âƒ£ Avaliando AgentCreator_Agent...")
    agent_creator_results = []
    for test_case in evaluator.test_cases["AgentCreator_Agent"]:
        print(f"   Teste: {test_case.name}")
        evaluation = evaluator.evaluate_agent("AgentCreator_Agent", test_case)
        agent_creator_results.append(evaluation)
        print(f"   Score: {evaluation.total_score:.2f}/10")
    
    # Avaliar OnboardingGuide_Agent
    print("\n2ï¸âƒ£ Avaliando OnboardingGuide_Agent...")
    onboarding_results = []
    for test_case in evaluator.test_cases["OnboardingGuide_Agent"]:
        print(f"   Teste: {test_case.name}")
        evaluation = evaluator.evaluate_agent("OnboardingGuide_Agent", test_case)
        onboarding_results.append(evaluation)
        print(f"   Score: {evaluation.total_score:.2f}/10")
    
    # Avaliar KotlinEntityCreator_Agent
    print("\n3ï¸âƒ£ Avaliando KotlinEntityCreator_Agent...")
    entity_creator_results = []
    for test_case in evaluator.test_cases["KotlinEntityCreator_Agent"]:
        print(f"   Teste: {test_case.name}")
        evaluation = evaluator.evaluate_agent("KotlinEntityCreator_Agent", test_case)
        entity_creator_results.append(evaluation)
        print(f"   Score: {evaluation.total_score:.2f}/10")
    
    # Calcular estatÃ­sticas
    print("\nğŸ“Š EstatÃ­sticas da AvaliaÃ§Ã£o:")
    print("-" * 30)
    
    all_results = {
        "AgentCreator_Agent": agent_creator_results,
        "OnboardingGuide_Agent": onboarding_results,
        "KotlinEntityCreator_Agent": entity_creator_results
    }
    
    for agent_id, results in all_results.items():
        if results:
            scores = [r.total_score for r in results]
            avg_score = sum(scores) / len(scores)
            min_score = min(scores)
            max_score = max(scores)
            
            print(f"\n{agent_id}:")
            print(f"  Score mÃ©dio: {avg_score:.2f}/10")
            print(f"  Range: {min_score:.2f} - {max_score:.2f}")
            print(f"  Total de testes: {len(results)}")
    
    # Gerar recomendaÃ§Ãµes
    print("\nğŸ’¡ RecomendaÃ§Ãµes:")
    print("-" * 20)
    
    for agent_id, results in all_results.items():
        if results:
            avg_score = sum(r.total_score for r in results) / len(results)
            
            if avg_score >= 8.0:
                status = "ğŸŸ¢ Excelente"
                recommendation = "Manter padrÃµes atuais"
            elif avg_score >= 7.0:
                status = "ğŸŸ¡ Bom"
                recommendation = "Melhorias moderadas recomendadas"
            elif avg_score >= 6.0:
                status = "ğŸŸ  Regular"
                recommendation = "Melhorias necessÃ¡rias"
            else:
                status = "ğŸ”´ CrÃ­tico"
                recommendation = "Melhoria urgente necessÃ¡ria"
            
            print(f"\n{agent_id}: {status}")
            print(f"  RecomendaÃ§Ã£o: {recommendation}")
    
    # Salvar resultados
    print("\nğŸ’¾ Salvando resultados...")
    evaluator._save_comprehensive_report(all_results)
    
    print(f"\nâœ… AvaliaÃ§Ã£o concluÃ­da!")
    print(f"ğŸ“ Resultados salvos em: {evaluator.results_dir}")
    
    # Mostrar prÃ³ximos passos
    print("\nğŸ¯ PrÃ³ximos Passos:")
    print("1. Revisar resultados detalhados nos arquivos JSON")
    print("2. Implementar melhorias baseadas nas recomendaÃ§Ãµes")
    print("3. Executar nova avaliaÃ§Ã£o para medir progresso")
    print("4. Estabelecer processo contÃ­nuo de monitoramento")
    
    print("\nğŸ”§ Para usar o sistema completo:")
    print(f"   cd {conductor_root}")
    print("   ./scripts/run_agent_evaluation.sh --full --report")

if __name__ == "__main__":
    main()
